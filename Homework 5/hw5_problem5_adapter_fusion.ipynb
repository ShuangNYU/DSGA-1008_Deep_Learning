{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw5_problem5_adapter_fusion.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HywR0S12rPDX"
      },
      "source": [
        "## Problem 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC9WZDt3xkPM"
      },
      "source": [
        "## For this question, fill in the TODO's "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmqCEJrUbi0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27910c56-20e0-407f-ab66-6cf2027463cb"
      },
      "source": [
        "!pip install git+https://github.com/adapter-hub/adapter-transformers.git\n",
        "!git clone https://github.com/huggingface/transformers\n",
        "!python transformers/utils/download_glue_data.py --tasks RTE\n",
        "\n",
        "import dataclasses\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, Optional\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, EvalPrediction, GlueDataset, GlueDataTrainingArguments, AutoModelWithHeads, AdapterType, AutoConfig, AutoModelForSequenceClassification\n",
        "from transformers import GlueDataTrainingArguments as DataTrainingArguments\n",
        "from transformers import (\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    glue_compute_metrics,\n",
        "    glue_tasks_num_labels,\n",
        "    set_seed,\n",
        ")\n",
        "\n",
        "model_name = \"bert-base-uncased\"# TODO \n",
        "\n",
        "\n",
        "# Refer to the notebook for training an adapter to write these. Set the number of epochs to 3, and learning rate to 5e-5. Rest of the hyperparameters can stay the same. \n",
        "\n",
        "data_args = GlueDataTrainingArguments(task_name=\"rte\", data_dir=\"./glue_data/RTE\")# TODO \n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    logging_steps=50, \n",
        "    per_device_train_batch_size=32, \n",
        "    per_device_eval_batch_size=64, \n",
        "    save_steps=1000,\n",
        "    evaluate_during_training=True,\n",
        "    output_dir=\"./models/rte\",\n",
        "    overwrite_output_dir=True,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    do_predict=True,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=3,\n",
        ")# TODO\n",
        "\n",
        "\n",
        "# TODO: Change this seed when re-running your code to report the mean and std dev\n",
        "set_seed(100)\n",
        "num_labels = glue_tasks_num_labels[data_args.task_name]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/adapter-hub/adapter-transformers.git\n",
            "  Cloning https://github.com/adapter-hub/adapter-transformers.git to /tmp/pip-req-build-z6wbfoil\n",
            "  Running command git clone -q https://github.com/adapter-hub/adapter-transformers.git /tmp/pip-req-build-z6wbfoil\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied (use --upgrade to upgrade): adapter-transformers==1.0.1 from git+https://github.com/adapter-hub/adapter-transformers.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from adapter-transformers==1.0.1) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from adapter-transformers==1.0.1) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from adapter-transformers==1.0.1) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from adapter-transformers==1.0.1) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from adapter-transformers==1.0.1) (0.8)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from adapter-transformers==1.0.1) (3.12.4)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from adapter-transformers==1.0.1) (0.9.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from adapter-transformers==1.0.1) (20.4)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from adapter-transformers==1.0.1) (0.1.91)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from adapter-transformers==1.0.1) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from adapter-transformers==1.0.1) (4.41.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->adapter-transformers==1.0.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->adapter-transformers==1.0.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->adapter-transformers==1.0.1) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->adapter-transformers==1.0.1) (2.10)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->adapter-transformers==1.0.1) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->adapter-transformers==1.0.1) (50.3.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->adapter-transformers==1.0.1) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->adapter-transformers==1.0.1) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->adapter-transformers==1.0.1) (7.1.2)\n",
            "Building wheels for collected packages: adapter-transformers\n",
            "  Building wheel for adapter-transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for adapter-transformers: filename=adapter_transformers-1.0.1-cp36-none-any.whl size=1325744 sha256=84a4383bb979172d45e5cedc58e3dd2d79186c1040bc9d12a5ed4912f828c745\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xipfkevu/wheels/22/eb/df/1c86c6a1b0323a74470d6a53db05b3b49ec79bce18d253ec38\n",
            "Successfully built adapter-transformers\n",
            "fatal: destination path 'transformers' already exists and is not an empty directory.\n",
            "Downloading and extracting RTE...\n",
            "\tCompleted!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/training_args.py:347: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JRwqK_-TbPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed0f264d-ad0b-4006-8db1-7d76d215912f"
      },
      "source": [
        "config = AutoConfig.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=num_labels,\n",
        "        finetuning_task=data_args.task_name,\n",
        "        cache_dir=\".\",\n",
        "    )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    cache_dir=\".\",\n",
        ")\n",
        "\n",
        "model = AutoModelWithHeads.from_pretrained(model_name, config=config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxY1ayPk340s"
      },
      "source": [
        "Now we have everything set up to load our AdapterFusion setup. \n",
        "\n",
        "First, you need to go to adapterhub.ml and explore the different adapters available. Choose any three adapters and load the three adapters. As we don't need their prediction heads, we pass with_head=False to the loading method. Next, we add a new fusion layer that combines all the adapters we've just loaded. Finally, we add a new classification head for our target task on top."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxE-ebUo37jm"
      },
      "source": [
        "# First, load the pre-trained adapters we want to fuse from Hub\n",
        "from transformers.adapter_config import PfeifferConfig\n",
        "\n",
        "model.load_adapter(\"nli/rte@ukp\", \"text_task\", config=PfeifferConfig(), load_as='rte', with_head=False)\n",
        "# TODO: load some more adapters. Choose the ones that you think will help RTE task after reading about the different tasks available and how big the datasets are.\n",
        "model.load_adapter(\"nli/multinli@ukp\", \"text_task\", config=PfeifferConfig(), load_as='multinli', with_head=False)\n",
        "model.load_adapter(\"nli/qnli@ukp\", \"text_task\", config=PfeifferConfig(), load_as='qnli', with_head=False)\n",
        "model.load_adapter(\"nli/sick@ukp\", \"text_task\", config=PfeifferConfig(), load_as='sick', with_head=False)\n",
        "\n",
        "# Add a fusion layer and tell the model to train fusion (freezes the rest of the weights) (here can either add the actual atsk adapter or not)\n",
        "model.add_fusion([\n",
        "        \"rte\",\n",
        "        # TODO: Add your other task names here for the adapters you chose\n",
        "        \"multinli\",\n",
        "        \"qnli\",\n",
        "        \"sick\"\n",
        "    ])\n",
        "\n",
        "# Add a classification head for our target task\n",
        "# TODO: Check the earlier notebook from Problem 5 to see how to add a classification head for your task.\n",
        "model.add_classification_head(\"rte\", num_labels=num_labels)\n",
        "model.set_active_adapters([[\"rte\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G4aQBqn3pkl"
      },
      "source": [
        "The last preparation step is to define and activate our adapter setup. Similar to train_adapter(), train_fusion() does two things: It freezes all weights of the model (including adapters!) except for the fusion layer and classification head. It also activates the given adapter setup to be used in very forward pass.\n",
        "\n",
        "The syntax for the adapter setup (which is also applied to other methods such as set_active_adapters()) works as follows:\n",
        "\n",
        "a single string is interpreted as a single adapter, \n",
        "a list of strings is interpreted as a stack of adapters,\n",
        "a nested list of strings is interpreted as a fusion of adapters. Here want to do Fusion so we use a nested list as follows. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLgNXXBi3sE7"
      },
      "source": [
        "adapter_setup = [\n",
        "                 [\"rte\"],\n",
        "        # TODO: Add your other adapter names here.\n",
        "        [\"multinli\"],\n",
        "        [\"qnli\"],\n",
        "        [\"sick\"]\n",
        "]\n",
        "model.train_fusion(adapter_setup)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS-UUxG3u2zN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0694a65-3741-4ea5-ec8f-b64228a07d6e"
      },
      "source": [
        "# Check out your training args\n",
        "print(training_args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TrainingArguments(output_dir='./models/rte', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=64, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3, max_steps=-1, warmup_steps=0, logging_dir='runs/Nov28_03-13-58_911fb740b770', logging_first_step=False, logging_steps=50, save_steps=1000, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=50, dataloader_num_workers=0, past_index=-1, run_name='./models/rte', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l128JFN3Vr9n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "4f0989f1-84a4-436b-cdfd-a6b06c681032"
      },
      "source": [
        "train_dataset = GlueDataset(data_args, tokenizer=tokenizer)\n",
        "eval_dataset = GlueDataset(data_args, tokenizer=tokenizer, mode=\"dev\")\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    return glue_compute_metrics(data_args.task_name, preds, p.label_ids)\n",
        "\n",
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        do_save_full_model=False,\n",
        "        do_save_adapter_fusion=True,\n",
        "    )\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/data/datasets/glue.py:77: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/data/processors/glue.py:521: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='234' max='234' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [234/234 00:26, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.612066</td>\n",
              "      <td>0.638109</td>\n",
              "      <td>0.660650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.589878</td>\n",
              "      <td>0.629485</td>\n",
              "      <td>0.675090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.570838</td>\n",
              "      <td>0.631773</td>\n",
              "      <td>0.667870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.561964</td>\n",
              "      <td>0.633062</td>\n",
              "      <td>0.667870</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/data/metrics/__init__.py:66: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/data/metrics/__init__.py:36: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 3.0, 'eval_acc': 0.6642599277978339, 'eval_loss': 0.6330525875091553}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltlyfASS37ur"
      },
      "source": [
        "#### Re-run with seed 200"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "n5W42RIT27v7",
        "outputId": "a8fc424b-0c7f-4fa7-9c2c-6f4d429553be"
      },
      "source": [
        "set_seed(200)\n",
        "num_labels = glue_tasks_num_labels[data_args.task_name]\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=num_labels,\n",
        "        finetuning_task=data_args.task_name,\n",
        "        cache_dir=\".\",\n",
        "    )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    cache_dir=\".\",\n",
        ")\n",
        "\n",
        "model = AutoModelWithHeads.from_pretrained(model_name, config=config)\n",
        "\n",
        "# First, load the pre-trained adapters we want to fuse from Hub\n",
        "from transformers.adapter_config import PfeifferConfig\n",
        "\n",
        "model.load_adapter(\"nli/rte@ukp\", \"text_task\", config=PfeifferConfig(), load_as='rte', with_head=False)\n",
        "# TODO: load some more adapters. Choose the ones that you think will help RTE task after reading about the different tasks available and how big the datasets are.\n",
        "model.load_adapter(\"nli/multinli@ukp\", \"text_task\", config=PfeifferConfig(), load_as='multinli', with_head=False)\n",
        "model.load_adapter(\"nli/qnli@ukp\", \"text_task\", config=PfeifferConfig(), load_as='qnli', with_head=False)\n",
        "model.load_adapter(\"nli/sick@ukp\", \"text_task\", config=PfeifferConfig(), load_as='sick', with_head=False)\n",
        "\n",
        "# Add a fusion layer and tell the model to train fusion (freezes the rest of the weights) (here can either add the actual atsk adapter or not)\n",
        "model.add_fusion([\n",
        "        \"rte\",\n",
        "        # TODO: Add your other task names here for the adapters you chose\n",
        "        \"multinli\",\n",
        "        \"qnli\",\n",
        "        \"sick\"\n",
        "    ])\n",
        "\n",
        "# Add a classification head for our target task\n",
        "# TODO: Check the earlier notebook from Problem 5 to see how to add a classification head for your task.\n",
        "model.add_classification_head(\"rte\", num_labels=num_labels)\n",
        "model.set_active_adapters([[\"rte\"]])\n",
        "\n",
        "adapter_setup = [\n",
        "                 [\"rte\"],\n",
        "        # TODO: Add your other adapter names here.\n",
        "        [\"multinli\"],\n",
        "        [\"qnli\"],\n",
        "        [\"sick\"]\n",
        "]\n",
        "model.train_fusion(adapter_setup)\n",
        "\n",
        "train_dataset = GlueDataset(data_args, tokenizer=tokenizer)\n",
        "eval_dataset = GlueDataset(data_args, tokenizer=tokenizer, mode=\"dev\")\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    return glue_compute_metrics(data_args.task_name, preds, p.label_ids)\n",
        "\n",
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        do_save_full_model=False,\n",
        "        do_save_adapter_fusion=True,\n",
        "    )\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/data/datasets/glue.py:77: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/data/processors/glue.py:521: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='234' max='234' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [234/234 00:27, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.609956</td>\n",
              "      <td>0.636222</td>\n",
              "      <td>0.664260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.591427</td>\n",
              "      <td>0.627984</td>\n",
              "      <td>0.660650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.572664</td>\n",
              "      <td>0.630336</td>\n",
              "      <td>0.664260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.561931</td>\n",
              "      <td>0.632014</td>\n",
              "      <td>0.667870</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/data/metrics/__init__.py:66: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/data/metrics/__init__.py:36: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 3.0, 'eval_acc': 0.6678700361010831, 'eval_loss': 0.6320486664772034}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5qXcFeQ4tfM"
      },
      "source": [
        "#### Re-run with seed 300"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "ijFjJN1B36K5",
        "outputId": "946079ff-5bba-47e3-9700-c3aa963aaafe"
      },
      "source": [
        "set_seed(300)\n",
        "num_labels = glue_tasks_num_labels[data_args.task_name]\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=num_labels,\n",
        "        finetuning_task=data_args.task_name,\n",
        "        cache_dir=\".\",\n",
        "    )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    cache_dir=\".\",\n",
        ")\n",
        "\n",
        "model = AutoModelWithHeads.from_pretrained(model_name, config=config)\n",
        "\n",
        "# First, load the pre-trained adapters we want to fuse from Hub\n",
        "from transformers.adapter_config import PfeifferConfig\n",
        "\n",
        "model.load_adapter(\"nli/rte@ukp\", \"text_task\", config=PfeifferConfig(), load_as='rte', with_head=False)\n",
        "# TODO: load some more adapters. Choose the ones that you think will help RTE task after reading about the different tasks available and how big the datasets are.\n",
        "model.load_adapter(\"nli/multinli@ukp\", \"text_task\", config=PfeifferConfig(), load_as='multinli', with_head=False)\n",
        "model.load_adapter(\"nli/qnli@ukp\", \"text_task\", config=PfeifferConfig(), load_as='qnli', with_head=False)\n",
        "model.load_adapter(\"nli/sick@ukp\", \"text_task\", config=PfeifferConfig(), load_as='sick', with_head=False)\n",
        "\n",
        "# Add a fusion layer and tell the model to train fusion (freezes the rest of the weights) (here can either add the actual atsk adapter or not)\n",
        "model.add_fusion([\n",
        "        \"rte\",\n",
        "        # TODO: Add your other task names here for the adapters you chose\n",
        "        \"multinli\",\n",
        "        \"qnli\",\n",
        "        \"sick\"\n",
        "    ])\n",
        "\n",
        "# Add a classification head for our target task\n",
        "# TODO: Check the earlier notebook from Problem 5 to see how to add a classification head for your task.\n",
        "model.add_classification_head(\"rte\", num_labels=num_labels)\n",
        "model.set_active_adapters([[\"rte\"]])\n",
        "\n",
        "adapter_setup = [\n",
        "                 [\"rte\"],\n",
        "        # TODO: Add your other adapter names here.\n",
        "        [\"multinli\"],\n",
        "        [\"qnli\"],\n",
        "        [\"sick\"]\n",
        "]\n",
        "model.train_fusion(adapter_setup)\n",
        "\n",
        "train_dataset = GlueDataset(data_args, tokenizer=tokenizer)\n",
        "eval_dataset = GlueDataset(data_args, tokenizer=tokenizer, mode=\"dev\")\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    return glue_compute_metrics(data_args.task_name, preds, p.label_ids)\n",
        "\n",
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        do_save_full_model=False,\n",
        "        do_save_adapter_fusion=True,\n",
        "    )\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/data/datasets/glue.py:77: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/data/processors/glue.py:521: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='234' max='234' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [234/234 00:27, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.609089</td>\n",
              "      <td>0.637168</td>\n",
              "      <td>0.664260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.591778</td>\n",
              "      <td>0.628941</td>\n",
              "      <td>0.671480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.573843</td>\n",
              "      <td>0.631170</td>\n",
              "      <td>0.671480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.563633</td>\n",
              "      <td>0.632800</td>\n",
              "      <td>0.667870</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/data/metrics/__init__.py:66: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/data/metrics/__init__.py:36: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 3.0, 'eval_acc': 0.6678700361010831, 'eval_loss': 0.632787823677063}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yMzV8nV4SIx"
      },
      "source": [
        "#### Mean and Standard Deviation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OQfyrJN4LmJ",
        "outputId": "5f48a7cf-da51-41e0-c87f-88cefe7d901e"
      },
      "source": [
        "best_val_acc = [0.675090, 0.667870, 0.671480]\n",
        "print('mean across 3 runs:', np.mean(best_val_acc))\n",
        "print('std across 3 runs:', np.std(best_val_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean across 3 runs: 0.67148\n",
            "std across 3 runs: 0.002947552657149093\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNOGC4hX5amp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}