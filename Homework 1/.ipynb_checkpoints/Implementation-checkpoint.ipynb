{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from mlp import MLP, mse_loss, bce_loss\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3776, 1.0338, 0.1749],\n",
       "        [0.5144, 1.4759, 0.1850]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[ 0.9455,  0.2088,  0.1070],\n",
    "        [ 0.0823,  0.6509,  0.1171]])\n",
    "b = torch.tensor([ 0.4321,  0.8250, 0.0679])\n",
    "a.add(b[None, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a > 0.5) * 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        linear_1_in_features,\n",
    "        linear_1_out_features,\n",
    "        f_function,\n",
    "        linear_2_in_features,\n",
    "        linear_2_out_features,\n",
    "        g_function\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            linear_1_in_features: the in features of first linear layer\n",
    "            linear_1_out_features: the out features of first linear layer\n",
    "            linear_2_in_features: the in features of second linear layer\n",
    "            linear_2_out_features: the out features of second linear layer\n",
    "            f_function: string for the f function: relu | sigmoid | identity\n",
    "            g_function: string for the g function: relu | sigmoid | identity\n",
    "        \"\"\"\n",
    "        self.f_function = f_function\n",
    "        self.g_function = g_function\n",
    "\n",
    "        self.parameters = dict(\n",
    "            W1 = torch.randn(linear_1_out_features, linear_1_in_features),\n",
    "            b1 = torch.randn(linear_1_out_features),\n",
    "            W2 = torch.randn(linear_2_out_features, linear_2_in_features),\n",
    "            b2 = torch.randn(linear_2_out_features),\n",
    "        )\n",
    "        self.grads = dict(\n",
    "            dJdW1 = torch.zeros(linear_1_out_features, linear_1_in_features),\n",
    "            dJdb1 = torch.zeros(linear_1_out_features),\n",
    "            dJdW2 = torch.zeros(linear_2_out_features, linear_2_in_features),\n",
    "            dJdb2 = torch.zeros(linear_2_out_features),\n",
    "        )\n",
    "\n",
    "        # put all the cache value you need in self.cache\n",
    "        self.cache = dict()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: tensor shape (batch_size, linear_1_in_features)\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward function\n",
    "        print(self.g_function)\n",
    "        activation_mapping = {'relu':nn.ReLU(), 'sigmoid':nn.Sigmoid(), 'identity':nn.Identity()}\n",
    "        z1 = torch.matmul(x, self.parameters['W1'].t()) + torch.ger(torch.ones(x.shape[0]), self.parameters['b1'])\n",
    "        z2 = activation_mapping[self.f_function](z1)\n",
    "        z3 = torch.matmul(z2, self.parameters['W2'].t()) + torch.ger(torch.ones(x.shape[0]), self.parameters['b2'])\n",
    "        y_hat = activation_mapping[self.g_function](z3)\n",
    "        self.cache['z1'] = z1\n",
    "        self.cache['z2'] = z2\n",
    "        self.cache['z3'] = z3\n",
    "        self.cache['x'] = x\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "    def backward(self, dJdy_hat):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)\n",
    "        \"\"\"\n",
    "        # TODO: Implement the backward function\n",
    "        def grad(function, input_):\n",
    "            grad_mappings = {'relu':(input_ > 0) * 1.0, \n",
    "                         'sigmoid':torch.exp(-input_)*(1+torch.exp(-input_))**(-2), \n",
    "                         'identity':torch.ones(input_.size())}\n",
    "            return grad_mappings[function]\n",
    "        \n",
    "        dz1dW1= self.cache['x'].t()\n",
    "        dz1db1= torch.ones(self.cache['x'].shape[0])\n",
    "        dz2dz1 = grad(self.f_function, self.cache['z1'])\n",
    "        dz3dz2 = self.parameters['W2']\n",
    "        dz3dW2= self.cache['z2'].t()\n",
    "        dz3db2= torch.ones(self.cache['z2'].shape[0])\n",
    "        dy_hatdz3 = grad(self.g_function, self.cache['z3'])\n",
    "        \n",
    "        self.grads['dJdW1'] = torch.matmul(dz1dW1, torch.matmul(dJdy_hat * dy_hatdz3, dz3dz2) * dz2dz1).t()\n",
    "        self.grads['dJdb1'] = torch.matmul(dz1db1, torch.matmul(dJdy_hat * dy_hatdz3, dz3dz2) * dz2dz1)\n",
    "        self.grads['dJdW2'] = torch.matmul(dz3dW2, dJdy_hat * dy_hatdz3).t()\n",
    "        self.grads['dJdb2'] = torch.matmul(dz3db2, dJdy_hat * dy_hatdz3)\n",
    "        \n",
    "\n",
    "    \n",
    "    def clear_grad_and_cache(self):\n",
    "        for grad in self.grads:\n",
    "            self.grads[grad].zero_()\n",
    "        self.cache = dict()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y: the label tensor (batch_size, linear_2_out_features)\n",
    "        y_hat: the prediction tensor (batch_size, linear_2_out_features)\n",
    "\n",
    "    Return:\n",
    "        J: scalar of loss\n",
    "        dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)\n",
    "    \"\"\"\n",
    "    # TODO: Implement the mse loss\n",
    "    loss = (0.5*(y_hat - y)**2).mean()\n",
    "    dJdy_hat = (y_hat - y)/(y.shape[0]*y.shape[1])\n",
    "\n",
    "    return loss, dJdy_hat\n",
    "\n",
    "def bce_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y_hat: the prediction tensor\n",
    "        y: the label tensor\n",
    "        \n",
    "    Return:\n",
    "        loss: scalar of loss\n",
    "        dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)\n",
    "    \"\"\"\n",
    "    # TODO: Implement the bce loss\n",
    "    \n",
    "    def bounded_log(y_hat):\n",
    "        result = torch.log(y_hat)\n",
    "        result[result < -100] = -100\n",
    "        return result\n",
    "    def bounded_yhat(y_hat):\n",
    "        result = torch.Tensor(y_hat.shape).copy_(y_hat)\n",
    "        result[result == 1] = 1- torch.exp(torch.tensor(-17.))\n",
    "        result[result == 0] = torch.exp(torch.tensor(-100.))\n",
    "        return result\n",
    "    loss = - (y * bounded_log(y_hat) + (1-y) * bounded_log(1-y_hat)).mean()\n",
    "#     dJdy_hat = (- y/y_hat + (1-y)/(1-y_hat)) * (((- y/y_hat + (1-y)/(1-y_hat)) > -100)*1.0)/(y.shape[0]*y.shape[1])\n",
    "#     y_hat = torch.exp(bounded_log(y_hat))\n",
    "    dJdy_hat = (- y/bounded_yhat(y_hat) + (1-y)/(1-bounded_yhat(y_hat)))/(y.shape[0]*y.shape[1])\n",
    "    return loss, dJdy_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9871e-01, 9.9999e-01],\n",
       "        [5.8329e-01, 6.0355e-08],\n",
       "        [1.0000e+00, 9.8905e-01],\n",
       "        [6.4327e-01, 9.7553e-01],\n",
       "        [1.0000e+00, 9.5003e-01],\n",
       "        [8.9108e-01, 9.9222e-01],\n",
       "        [9.9995e-01, 9.9263e-01],\n",
       "        [9.5898e-01, 3.8934e-01],\n",
       "        [1.0000e+00, 1.0000e+00],\n",
       "        [1.0000e+00, 9.9999e-01]])"
      ]
     },
     "execution_count": 889,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.copy_(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid\n"
     ]
    }
   ],
   "source": [
    "net = MLP(\n",
    "    linear_1_in_features=2,\n",
    "    linear_1_out_features=20,\n",
    "    f_function='relu',\n",
    "    linear_2_in_features=20,\n",
    "    linear_2_out_features=2,\n",
    "    g_function='sigmoid'\n",
    ")\n",
    "\n",
    "x = torch.randn(10, 2)\n",
    "# y = ((torch.randn(10) > 0.5) * 1.0).unsqueeze(-1)\n",
    "y = ((torch.randn(10, 2) > 0.5) * 1.0)\n",
    "\n",
    "net.clear_grad_and_cache()\n",
    "y_hat = net.forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 940,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(((y_hat != 0)&(y_hat != 1))*1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "metadata": {},
   "outputs": [],
   "source": [
    "J, dJdy_hat = bce_loss(y, y_hat)\n",
    "net.backward(dJdy_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True],\n",
       "        [True, True],\n",
       "        [True, True],\n",
       "        [True, True],\n",
       "        [True, True],\n",
       "        [True, True],\n",
       "        [True, True],\n",
       "        [True, True],\n",
       "        [True, True],\n",
       "        [True, True]])"
      ]
     },
     "execution_count": 942,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_hat!=0)&(y_hat!=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_autograd = nn.Sequential(\n",
    "    OrderedDict([\n",
    "        ('linear1', nn.Linear(2, 20)),\n",
    "        ('relu', nn.ReLU()),\n",
    "        ('linear2', nn.Linear(20, 2)),\n",
    "        ('sigmoid', nn.Sigmoid())\n",
    "    ])\n",
    ")\n",
    "net_autograd.linear1.weight.data = net.parameters['W1']\n",
    "net_autograd.linear1.bias.data = net.parameters['b1']\n",
    "net_autograd.linear2.weight.data = net.parameters['W2']\n",
    "net_autograd.linear2.bias.data = net.parameters['b2']\n",
    "\n",
    "y_hat_autograd = net_autograd(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True],\n",
       "        [True, True],\n",
       "        [True, True],\n",
       "        [True, True],\n",
       "        [True, True],\n",
       "        [True, True],\n",
       "        [True, True],\n",
       "        [True, True],\n",
       "        [True, True],\n",
       "        [True, True]])"
      ]
     },
     "execution_count": 944,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat == y_hat_autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-inf)"
      ]
     },
     "execution_count": 951,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 0.8\n",
    "a_hat = torch.exp(torch.tensor(-100.))\n",
    "-(a/a_hat+(1-a)/(1-a_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 965,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 965,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(torch.tensor(-100.)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 966,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-torch.exp(torch.tensor(-100.)) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6914)"
      ]
     },
     "execution_count": 945,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J, dJdy_hat = bce_loss(y, y_hat)\n",
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6914, grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 946,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J_autograd = F.binary_cross_entropy(y_hat_autograd, y)\n",
    "J_autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dJdW1 tensor([[ 0.0000,  0.0000],\n",
      "        [ 0.0550,  0.0297],\n",
      "        [-0.0553,  0.1047],\n",
      "        [ 0.0041, -0.0031],\n",
      "        [-0.0238,  0.1432],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0625, -0.0298],\n",
      "        [-0.2098,  0.0952],\n",
      "        [-0.0894,  0.0395],\n",
      "        [-0.0758,  0.2430],\n",
      "        [ 0.0062, -0.0024],\n",
      "        [ 0.0385,  0.0049],\n",
      "        [ 0.1133,  0.2157],\n",
      "        [-0.1328,  0.0198],\n",
      "        [ 0.0180, -0.0282],\n",
      "        [-0.0609,  0.4745],\n",
      "        [ 0.0222, -0.0826],\n",
      "        [ 0.0090,  0.0054],\n",
      "        [-0.2402,  0.0817],\n",
      "        [-0.1345,  0.0529]])\n",
      "tensor([[ 0.0000,  0.0000],\n",
      "        [ 0.0550,  0.0297],\n",
      "        [-0.0553,  0.1047],\n",
      "        [ 0.0041, -0.0031],\n",
      "        [-0.0238,  0.1432],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0625, -0.0298],\n",
      "        [-0.2098,  0.0952],\n",
      "        [-0.0894,  0.0395],\n",
      "        [-0.0758,  0.2430],\n",
      "        [ 0.0062, -0.0024],\n",
      "        [ 0.0385,  0.0049],\n",
      "        [ 0.1133,  0.2157],\n",
      "        [-0.1328,  0.0198],\n",
      "        [ 0.0180, -0.0282],\n",
      "        [-0.0609,  0.4745],\n",
      "        [ 0.0222, -0.0826],\n",
      "        [ 0.0090,  0.0054],\n",
      "        [-0.2402,  0.0817],\n",
      "        [-0.1345,  0.0529]])\n",
      "dJdb1 tensor([ 0.0000, -0.0969,  0.1258,  0.0058,  0.1301,  0.0000,  0.0872,  0.1920,\n",
      "         0.0845,  0.4513, -0.0033, -0.0547,  0.2084, -0.1018, -0.0262,  0.8093,\n",
      "        -0.1236, -0.0167,  0.2435,  0.1280])\n",
      "tensor([ 0.0000, -0.0969,  0.1258,  0.0058,  0.1300,  0.0000,  0.0872,  0.1920,\n",
      "         0.0845,  0.4513, -0.0033, -0.0547,  0.2084, -0.1018, -0.0262,  0.8093,\n",
      "        -0.1236, -0.0167,  0.2435,  0.1280])\n",
      "dJdW2 tensor([[ 0.0000e+00,  2.3607e-03, -2.8167e-01,  1.1627e-02, -1.0935e-01,\n",
      "          0.0000e+00, -2.4442e-01, -7.3003e-02, -4.4967e-02, -3.2382e-01,\n",
      "          1.3998e-03, -1.7935e-01, -3.3818e-02,  6.7431e-05, -1.4400e-01,\n",
      "         -7.6900e-01, -3.1009e-01,  6.6990e-03, -1.2552e-01, -9.7694e-03],\n",
      "        [ 0.0000e+00,  7.4606e-03,  2.7227e-01,  9.0108e-02,  8.9360e-02,\n",
      "          0.0000e+00,  3.3927e-01,  8.5223e-02,  2.6891e-02,  2.5382e-01,\n",
      "          4.5867e-03,  2.1001e-01,  6.7134e-02,  1.9827e-02,  2.2096e-01,\n",
      "          1.1865e+00,  4.4133e-01,  1.9117e-02,  1.3815e-01,  2.6248e-02]])\n",
      "tensor([[ 0.0000e+00,  2.3607e-03, -2.8167e-01,  1.1627e-02, -1.0935e-01,\n",
      "          0.0000e+00, -2.4442e-01, -7.3003e-02, -4.4967e-02, -3.2382e-01,\n",
      "          1.3998e-03, -1.7935e-01, -3.3818e-02,  6.7431e-05, -1.4400e-01,\n",
      "         -7.6900e-01, -3.1009e-01,  6.6990e-03, -1.2552e-01, -9.7694e-03],\n",
      "        [ 0.0000e+00,  7.4606e-03,  2.7227e-01,  9.0107e-02,  8.9361e-02,\n",
      "          0.0000e+00,  3.3928e-01,  8.5224e-02,  2.6892e-02,  2.5382e-01,\n",
      "          4.5867e-03,  2.1001e-01,  6.7134e-02,  1.9827e-02,  2.2096e-01,\n",
      "          1.1865e+00,  4.4133e-01,  1.9117e-02,  1.3815e-01,  2.6248e-02]])\n",
      "dJdb2 tensor([-0.2408,  0.3893])\n",
      "tensor([-0.2408,  0.3893])\n",
      "tensor(9.6411e-06)\n",
      "tensor(9.0911e-06)\n",
      "tensor(1.9238e-05)\n",
      "tensor(4.5896e-06)\n"
     ]
    }
   ],
   "source": [
    "# J_autograd = 0.5 * F.binary_cross_entropy(y_hat_autograd, y)\n",
    "J_autograd = F.binary_cross_entropy(y_hat_autograd, y)\n",
    "\n",
    "net_autograd.zero_grad()\n",
    "J_autograd.backward()\n",
    "\n",
    "print('dJdW1', net.grads['dJdW1'])\n",
    "print(net_autograd.linear1.weight.grad.data)\n",
    "print('dJdb1', net.grads['dJdb1'])\n",
    "print(net_autograd.linear1.bias.grad.data)\n",
    "print('dJdW2', net.grads['dJdW2'])\n",
    "print(net_autograd.linear2.weight.grad.data)\n",
    "print('dJdb2', net.grads['dJdb2'])\n",
    "print(net_autograd.linear2.bias.grad.data)\n",
    "\n",
    "print((net_autograd.linear1.weight.grad.data - net.grads['dJdW1']).norm())\n",
    "print((net_autograd.linear1.bias.grad.data - net.grads['dJdb1']).norm())\n",
    "print((net_autograd.linear2.weight.grad.data - net.grads['dJdW2']).norm())\n",
    "print((net_autograd.linear2.bias.grad.data - net.grads['dJdb2']).norm())\n",
    "#------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid\n"
     ]
    }
   ],
   "source": [
    "net = MLP(\n",
    "    linear_1_in_features=10,\n",
    "    linear_1_out_features=20,\n",
    "    f_function='relu',\n",
    "    linear_2_in_features=20,\n",
    "    linear_2_out_features=1,\n",
    "    g_function='sigmoid'\n",
    ")\n",
    "x = torch.randn(10, 10)\n",
    "y = ((torch.randn(10) > 0.5) * 1.0).unsqueeze(-1)\n",
    "# y = ((torch.randn(10, 2) > 0.5) * 1.0)\n",
    "net.clear_grad_and_cache()\n",
    "y_hat = net.forward(x)\n",
    "J, dJdy_hat = bce_loss(y, y_hat)\n",
    "net.backward(dJdy_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------\n",
    "# check the result with autograd\n",
    "net_autograd = nn.Sequential(\n",
    "    OrderedDict([\n",
    "        ('linear1', nn.Linear(10, 20)),\n",
    "        ('relu', nn.ReLU()),\n",
    "        ('linear2', nn.Linear(20, 1)),\n",
    "        ('sigmoid', nn.Sigmoid())\n",
    "    ])\n",
    ")\n",
    "net_autograd.linear1.weight.data = net.parameters['W1']\n",
    "net_autograd.linear1.bias.data = net.parameters['b1']\n",
    "net_autograd.linear2.weight.data = net.parameters['W2']\n",
    "net_autograd.linear2.bias.data = net.parameters['b2']\n",
    "\n",
    "y_hat_autograd = net_autograd(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True]])"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat == y_hat_autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2830, grad_fn=<BinaryCrossEntropyBackward>) tensor(1.2830)\n",
      "dJdW1 tensor([[ 0.4260, -0.0992,  0.2537, -0.2614, -0.0274, -0.0733, -0.1926,  0.1593,\n",
      "         -0.2237, -0.3298],\n",
      "        [ 0.0373,  0.0471,  0.1268,  0.1263,  0.0738, -0.0024,  0.1056,  0.1270,\n",
      "         -0.1270, -0.1124],\n",
      "        [-0.0701,  0.0574, -0.0127,  0.1146,  0.0330,  0.0150,  0.0973, -0.0043,\n",
      "          0.0015,  0.0265],\n",
      "        [-0.0417,  0.0118,  0.0008,  0.0162, -0.0303, -0.0150,  0.0142, -0.0070,\n",
      "          0.0174,  0.0528],\n",
      "        [-0.1968, -0.0140, -0.0100,  0.0015,  0.1446, -0.0044,  0.0382,  0.1388,\n",
      "         -0.1883,  0.1233],\n",
      "        [ 0.0036, -0.0008, -0.0015, -0.0057,  0.0096,  0.0026,  0.0109, -0.0091,\n",
      "         -0.0071, -0.0018],\n",
      "        [-0.0369,  0.0159,  0.0205,  0.0301,  0.0507, -0.0007,  0.0381,  0.0519,\n",
      "         -0.0738,  0.0067],\n",
      "        [ 0.0006, -0.0127, -0.0020, -0.0030,  0.0013, -0.0144, -0.0009, -0.0034,\n",
      "          0.0100, -0.0004],\n",
      "        [ 0.0298, -0.0043,  0.1684,  0.1206,  0.1272, -0.0117,  0.1269,  0.1812,\n",
      "         -0.1708, -0.1386],\n",
      "        [-0.2603,  0.0797, -0.0288,  0.1798,  0.1688,  0.0219,  0.1894,  0.0952,\n",
      "         -0.1463,  0.1359],\n",
      "        [-0.1748,  0.0211, -0.0277,  0.0117,  0.1241, -0.0025,  0.0487,  0.0965,\n",
      "         -0.1694,  0.1218],\n",
      "        [-0.0467,  0.0132,  0.0009,  0.0181, -0.0339, -0.0168,  0.0159, -0.0078,\n",
      "          0.0195,  0.0591],\n",
      "        [ 0.2050, -0.0672,  0.0211, -0.1439, -0.1272, -0.0212, -0.1435, -0.0793,\n",
      "          0.1147, -0.1070],\n",
      "        [-0.0674,  0.0206, -0.0074,  0.0465,  0.0437,  0.0057,  0.0490,  0.0246,\n",
      "         -0.0378,  0.0352],\n",
      "        [ 0.1042, -0.0867,  0.0174, -0.1707, -0.0430, -0.0237, -0.1365,  0.0010,\n",
      "         -0.0036, -0.0397],\n",
      "        [-0.2745,  0.0546, -0.1049,  0.0976,  0.1021,  0.0290,  0.1015,  0.0207,\n",
      "         -0.0570,  0.2023],\n",
      "        [ 0.1813, -0.0303,  0.1483, -0.1195, -0.0666, -0.0540, -0.0989,  0.0915,\n",
      "         -0.1050, -0.1133],\n",
      "        [-0.0118, -0.0402,  0.0150, -0.0148,  0.0219, -0.0078, -0.0047,  0.0286,\n",
      "         -0.0108, -0.0038],\n",
      "        [ 0.2087,  0.0156,  0.0120,  0.0025, -0.1628,  0.0027, -0.0494, -0.1426,\n",
      "          0.2078, -0.1312],\n",
      "        [-0.1924,  0.0233, -0.0306,  0.0131,  0.1367, -0.0027,  0.0535,  0.1065,\n",
      "         -0.1864,  0.1342]])\n",
      "tensor([[ 0.4260, -0.0992,  0.2537, -0.2614, -0.0274, -0.0733, -0.1926,  0.1593,\n",
      "         -0.2237, -0.3298],\n",
      "        [ 0.0373,  0.0471,  0.1268,  0.1263,  0.0738, -0.0024,  0.1056,  0.1270,\n",
      "         -0.1270, -0.1124],\n",
      "        [-0.0701,  0.0574, -0.0127,  0.1146,  0.0330,  0.0150,  0.0973, -0.0043,\n",
      "          0.0015,  0.0265],\n",
      "        [-0.0417,  0.0118,  0.0008,  0.0162, -0.0303, -0.0150,  0.0142, -0.0070,\n",
      "          0.0174,  0.0528],\n",
      "        [-0.1968, -0.0140, -0.0100,  0.0015,  0.1446, -0.0044,  0.0382,  0.1388,\n",
      "         -0.1883,  0.1233],\n",
      "        [ 0.0036, -0.0008, -0.0015, -0.0057,  0.0096,  0.0026,  0.0109, -0.0091,\n",
      "         -0.0071, -0.0018],\n",
      "        [-0.0369,  0.0159,  0.0205,  0.0301,  0.0507, -0.0007,  0.0381,  0.0519,\n",
      "         -0.0738,  0.0067],\n",
      "        [ 0.0006, -0.0127, -0.0020, -0.0030,  0.0013, -0.0144, -0.0009, -0.0034,\n",
      "          0.0100, -0.0004],\n",
      "        [ 0.0298, -0.0043,  0.1684,  0.1206,  0.1272, -0.0117,  0.1269,  0.1812,\n",
      "         -0.1708, -0.1386],\n",
      "        [-0.2603,  0.0797, -0.0288,  0.1798,  0.1688,  0.0219,  0.1894,  0.0952,\n",
      "         -0.1463,  0.1359],\n",
      "        [-0.1748,  0.0211, -0.0277,  0.0117,  0.1241, -0.0025,  0.0487,  0.0965,\n",
      "         -0.1694,  0.1218],\n",
      "        [-0.0467,  0.0132,  0.0009,  0.0181, -0.0339, -0.0168,  0.0159, -0.0078,\n",
      "          0.0195,  0.0591],\n",
      "        [ 0.2050, -0.0672,  0.0211, -0.1439, -0.1272, -0.0212, -0.1435, -0.0793,\n",
      "          0.1147, -0.1070],\n",
      "        [-0.0674,  0.0206, -0.0074,  0.0465,  0.0437,  0.0057,  0.0490,  0.0246,\n",
      "         -0.0378,  0.0352],\n",
      "        [ 0.1042, -0.0867,  0.0174, -0.1707, -0.0430, -0.0237, -0.1365,  0.0010,\n",
      "         -0.0036, -0.0397],\n",
      "        [-0.2745,  0.0546, -0.1049,  0.0976,  0.1021,  0.0290,  0.1015,  0.0207,\n",
      "         -0.0570,  0.2023],\n",
      "        [ 0.1813, -0.0303,  0.1483, -0.1195, -0.0666, -0.0540, -0.0989,  0.0915,\n",
      "         -0.1050, -0.1133],\n",
      "        [-0.0118, -0.0402,  0.0150, -0.0148,  0.0219, -0.0078, -0.0047,  0.0286,\n",
      "         -0.0108, -0.0038],\n",
      "        [ 0.2087,  0.0156,  0.0120,  0.0025, -0.1628,  0.0027, -0.0494, -0.1426,\n",
      "          0.2078, -0.1312],\n",
      "        [-0.1924,  0.0233, -0.0306,  0.0131,  0.1367, -0.0027,  0.0535,  0.1065,\n",
      "         -0.1864,  0.1342]])\n",
      "dJdb1 tensor([ 0.1666,  0.0905, -0.0142, -0.0196,  0.1081, -0.0108,  0.0390,  0.0057,\n",
      "         0.1246,  0.0548,  0.0797, -0.0219, -0.0453,  0.0142,  0.0165,  0.0017,\n",
      "         0.0730,  0.0201, -0.1082,  0.0879])\n",
      "tensor([ 0.1666,  0.0905, -0.0142, -0.0196,  0.1081, -0.0108,  0.0390,  0.0057,\n",
      "         0.1246,  0.0548,  0.0797, -0.0219, -0.0453,  0.0142,  0.0165,  0.0017,\n",
      "         0.0730,  0.0201, -0.1082,  0.0879])\n",
      "dJdW2 tensor([[-0.3207,  0.1482, -0.2070, -0.0176,  0.3315,  0.0108,  0.8238,  0.0043,\n",
      "          0.0995,  0.6843,  0.3554, -0.0992,  0.3316,  0.1081,  0.1410,  0.1706,\n",
      "         -0.0219,  0.0486,  0.6942,  0.0198]])\n",
      "tensor([[-0.3207,  0.1482, -0.2070, -0.0176,  0.3315,  0.0108,  0.8238,  0.0043,\n",
      "          0.0995,  0.6843,  0.3554, -0.0992,  0.3316,  0.1081,  0.1410,  0.1706,\n",
      "         -0.0219,  0.0486,  0.6942,  0.0198]])\n",
      "dJdb2 tensor([0.0950])\n",
      "tensor([0.0950])\n",
      "tensor(5.9347e-06)\n",
      "tensor(1.4407e-06)\n",
      "tensor(9.2563e-06)\n",
      "tensor(7.4506e-07)\n"
     ]
    }
   ],
   "source": [
    "# J_autograd = 0.5 * F.mse_loss(y_hat_autograd, y)\n",
    "J_autograd = F.binary_cross_entropy(y_hat_autograd, y)\n",
    "\n",
    "net_autograd.zero_grad()\n",
    "J_autograd.backward()\n",
    "\n",
    "print(J_autograd, J)\n",
    "\n",
    "print('dJdW1', net.grads['dJdW1'])\n",
    "print(net_autograd.linear1.weight.grad.data)\n",
    "print('dJdb1', net.grads['dJdb1'])\n",
    "print(net_autograd.linear1.bias.grad.data)\n",
    "print('dJdW2', net.grads['dJdW2'])\n",
    "print(net_autograd.linear2.weight.grad.data)\n",
    "print('dJdb2', net.grads['dJdb2'])\n",
    "print(net_autograd.linear2.bias.grad.data)\n",
    "\n",
    "print((net_autograd.linear1.weight.grad.data - net.grads['dJdW1']).norm())\n",
    "print((net_autograd.linear1.bias.grad.data - net.grads['dJdb1']).norm())\n",
    "print((net_autograd.linear2.weight.grad.data - net.grads['dJdW2']).norm())\n",
    "print((net_autograd.linear2.bias.grad.data - net.grads['dJdb2']).norm())\n",
    "#------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid\n",
      "tensor([[True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True]])\n",
      "tensor(1.3618) tensor(1.3618, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "dJdW1 tensor([[ 5.2330e-08,  4.5411e-08,  5.1723e-08,  3.3947e-10,  1.6062e-08],\n",
      "        [ 9.4287e-02,  2.6134e-02, -2.2243e-01, -2.0428e-01, -5.3525e-03],\n",
      "        [-2.5902e-01,  2.8303e-01, -3.1989e-02,  3.0906e-02,  1.1835e-02],\n",
      "        [ 2.5386e-02, -3.3068e-02, -1.0336e-03, -1.4944e-02,  1.1282e-02],\n",
      "        [ 1.1485e-01, -1.0632e-01, -6.9186e-02, -1.2111e-01, -8.6954e-02],\n",
      "        [ 1.7376e-02, -9.4481e-03, -1.8985e-02, -2.5139e-02, -8.4100e-03],\n",
      "        [ 2.6013e-02,  4.4231e-02, -1.1121e-01, -1.2943e-01, -2.9918e-02],\n",
      "        [ 2.7404e-03, -4.2974e-03,  9.7438e-04, -4.3676e-04, -8.6682e-04],\n",
      "        [-1.4892e-01,  8.0971e-02,  1.6270e-01,  2.1544e-01,  7.2075e-02],\n",
      "        [-4.0202e-02,  2.4951e-02,  3.9676e-02,  6.2537e-02,  2.2434e-02]])\n",
      "tensor([[-0.3241, -0.0307, -0.1974, -0.0232, -0.0153],\n",
      "        [ 0.0943,  0.0261, -0.2224, -0.2043, -0.0054],\n",
      "        [-0.2590,  0.2830, -0.0320,  0.0309,  0.0118],\n",
      "        [-0.0265, -0.0380, -0.0327, -0.0187,  0.0088],\n",
      "        [-0.1732, -0.1336, -0.2446, -0.1417, -0.1006],\n",
      "        [ 0.0174, -0.0094, -0.0190, -0.0251, -0.0084],\n",
      "        [-0.1364,  0.0289, -0.2101, -0.1410, -0.0376],\n",
      "        [-0.0042, -0.0050, -0.0033, -0.0009, -0.0012],\n",
      "        [-0.1489,  0.0810,  0.1627,  0.2154,  0.0721],\n",
      "        [ 0.0326,  0.0318,  0.0840,  0.0677,  0.0259]])\n",
      "dJdb1 tensor([-2.6670e-08, -2.0256e-01, -2.7536e-01,  1.0763e-02,  2.0274e-02,\n",
      "        -9.5986e-03, -1.2700e-01,  2.9235e-03,  8.2261e-02,  2.6444e-02])\n",
      "tensor([ 0.1489, -0.2026, -0.2754,  0.0346,  0.1526, -0.0096, -0.0524,  0.0061,\n",
      "         0.0823, -0.0070])\n",
      "dJdW2 tensor([[ 5.7690e-08,  7.8645e-02, -1.1995e-01, -3.6858e-01, -1.8802e-01,\n",
      "          2.3736e-01,  2.1205e-01, -1.4271e-02,  1.4727e-01,  3.4760e-01]])\n",
      "tensor([[-0.2277,  0.0786, -0.1199, -0.5250, -0.2908,  0.2374, -0.1997, -0.0570,\n",
      "          0.1473,  0.3111]])\n",
      "dJdb2 tensor([0.0544])\n",
      "tensor([-0.0455])\n",
      "tensor(0.5555)\n",
      "tensor(0.2167)\n",
      "tensor(0.5095)\n",
      "tensor(0.1000)\n"
     ]
    }
   ],
   "source": [
    "net = MLP(\n",
    "    linear_1_in_features=5,\n",
    "    linear_1_out_features=10,\n",
    "    f_function='relu',\n",
    "    linear_2_in_features=10,\n",
    "    linear_2_out_features=1,\n",
    "    g_function='sigmoid'\n",
    ")\n",
    "x = torch.randn(10, 5)\n",
    "y = ((torch.randn(10) > 0.5) * 1.0).unsqueeze(-1)\n",
    "# y = ((torch.randn(10, 2) > 0.5) * 1.0)\n",
    "net.clear_grad_and_cache()\n",
    "y_hat = net.forward(x)\n",
    "# J, dJdy_hat = mse_loss(y, y_hat)\n",
    "J, dJdy_hat = bce_loss(y, y_hat)\n",
    "net.backward(dJdy_hat)\n",
    "#------------------------------------------------\n",
    "# check the result with autograd\n",
    "net_autograd = nn.Sequential(\n",
    "    OrderedDict([\n",
    "        ('linear1', nn.Linear(5, 10)),\n",
    "        ('relu', nn.ReLU()),\n",
    "        ('linear2', nn.Linear(10, 1)),\n",
    "        ('sigmoid', nn.Sigmoid())\n",
    "    ])\n",
    ")\n",
    "net_autograd.linear1.weight.data = net.parameters['W1']\n",
    "net_autograd.linear1.bias.data = net.parameters['b1']\n",
    "net_autograd.linear2.weight.data = net.parameters['W2']\n",
    "net_autograd.linear2.bias.data = net.parameters['b2']\n",
    "\n",
    "y_hat_autograd = net_autograd(x)\n",
    "\n",
    "# J_autograd = 0.5 * F.mse_loss(y_hat_autograd, y)\n",
    "J_autograd = F.binary_cross_entropy(y_hat_autograd, y)\n",
    "\n",
    "net_autograd.zero_grad()\n",
    "J_autograd.backward()\n",
    "\n",
    "print(y_hat == y_hat_autograd)\n",
    "print(J, J_autograd)\n",
    "\n",
    "print('dJdW1', net.grads['dJdW1'])\n",
    "print(net_autograd.linear1.weight.grad.data)\n",
    "print('dJdb1', net.grads['dJdb1'])\n",
    "print(net_autograd.linear1.bias.grad.data)\n",
    "print('dJdW2', net.grads['dJdW2'])\n",
    "print(net_autograd.linear2.weight.grad.data)\n",
    "print('dJdb2', net.grads['dJdb2'])\n",
    "print(net_autograd.linear2.bias.grad.data)\n",
    "\n",
    "print((net_autograd.linear1.weight.grad.data - net.grads['dJdW1']).norm())\n",
    "print((net_autograd.linear1.bias.grad.data - net.grads['dJdb1']).norm())\n",
    "print((net_autograd.linear2.weight.grad.data - net.grads['dJdW2']).norm())\n",
    "print((net_autograd.linear2.bias.grad.data - net.grads['dJdb2']).norm())\n",
    "#------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
